{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Simple Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong series này, chúng ta sẽ xây dựng một mô hình học máy phân biệt cảm xúc (ví dụ: phát hiện xem một câu bình luận là tiêu cực (neg) hay tích cực (pos)) sử dụng pytorch và torchtext. Tập dữ liệu sử dụng ở đây là: [IMDb Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong Notebook đầu tiên này, chúng ta khởi động bằng việc đọc hiểu các khái niệm chung, chưa cần quan tâm quá nhiều đến kết quả. Các Notebook sau sẽ xây dựng dựa trên những kiến thức nền tảng này và cải thiện kết quả."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ sử dụng **recurrent neural network** (RNN) bởi vì đây là phương án phổ biến trong bài toán phân tích chuỗi (analysing sequences). Một RNN nhận đầu vào là một chuỗi các từ, X = {$x_1$, ..., $x_T$} mỗi lần, và sinh ra một **_hidden state: h_**, cho mỗi từ. Chúng ta sử dụng RNN một cách tuần tự bằng cách đưa từ hiện tại $x_t$ và **trạng thái ẩn** của từ phía trước, $h_{t-1}$ để sinh ra **trạng thái ẩn tiếp theo: $h_t$** <br>\n",
    "* $h_t$ = RNN($x_t$ $h_{t-1}$) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi thu được trạng thái ẩn cuối cùng: $h_T$ (bằng cách đưa từ cuối cùng của câu $x_T$ vào RNN) chúng ta đưa nó qua một linear layer f (hay còn gọi là fully connected layer), để cho ra kết quả dự đoán, $\\hat{y}$ = f($h_T$). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN + Linear layer](./image/1_rnn_figure1.PNG \"RNN + Linear layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "* Màu vàng là RNN layer, màu trắng là Linear layer.\n",
    "* Ta sử dụng cùng một RNN cho tất cả các từ, do đó chúng có cùng tham số (hoặc gọi là chia sẻ tham số).\n",
    "* Trạng thái ẩn $h_0$ có dạng tensor, được khởi tạo bằng 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một trong những khái niệm cơ sở trong TorchText là ***Field***. Nó định nghĩa các mà dữ liệu được xử lý. Trong nhiệm vụ phân tích cảm xúc, dữ liệu bao gồm phần đánh giá và phần nhãn (pos hoặc neg) đều ở dạng chuỗi văn bản. <br>\n",
    "Các tham số trong ***Field*** định nghĩa cách mà dữ liệu được xử lý. <br>\n",
    "Chúng ta sử dụng trường ***TEXT*** để định nghĩa cách xử lý phần đánh giá và trường ***LABEL*** xác định cách xử lý phần nhãn. <br>\n",
    "Trường ***TEXT*** có tham số ***tokenize='spacy'*** định nghĩa \"tokenization\" (tokenization là việc chia chuỗi văn bản thành các \"tokens\" riêng biệt) được xử lý bằng tokenizer trong thư viện [spaCy](https://spacy.io/). Nếu không truyền đối số tokenize, chuỗi sẽ mặc định được chia thành các tokens, ngăn cách bằng dấu cách (space). Bên cạnh đó, chúng ta cũng cần định nghĩa đối số ***tokenizer_language***, giúp thông báo torchtext sẽ sử dụng thư viện ngông ngữ nào. <br>\n",
    "***LABEL*** được định nghĩa bởi ***LabelField***, một trường con của ***Field***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1fd8e84f508>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                    tokenizer_language = 'en_core_web_sm')\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download dataset IMDb, gồm 50.000 review phim.\n",
    "2. Do tập dữ liệu chỉ có train và test, ta cần chia thêm tập validation từ tập train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  25000\n",
      "Number of testing examples:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \", len(train_data))\n",
    "print(\"Number of testing examples: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 80% train, 20% validate\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED), split_ratio = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  20000\n",
      "Number of validation examples:  5000\n",
      "Number of testing examples:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \", len(train_data))\n",
    "print(\"Number of validation examples: \", len(valid_data))\n",
    "print(\"Number of testing examples: \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta sẽ xây dựng ***từ điển*** (_vocabulary_). <br>\n",
    "Từ điển là một bảng tra cứu (lookup table), mỗi từ trong tập dữ liệu sẽ có một _chỉ số_ trong bảng. <br>\n",
    "> Tại sao phải xây dựng từ điển? <br>\n",
    "=> Câu trả lời là các mô hình học máy chỉ có thể làm việc với kiểu dữ liệu số, không thể làm việc với các kiểu dữ liệu khác. Mỗi một _chỉ số_ trong bảng tra cứu tạo nên một _one-hot vector_ cho mỗi từ tương ứng. Một _one-hot vector_ là vector có các phần từ bằng 0, duy nhất có một phần từ bằng 1 và có kích thước bằng với kích thước của từ điển - ký hiệu là **V** (kích thước từ điển là số unique word có trong từ điển). <br>\n",
    "Ví dụ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![One hot vector example](./image/1_onehot_vector_example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như ví dụ trên, kích thước từ điển bằng 4, do đó kích thước của one-hot vector là 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tuy nhiên, số lượng unique words trong tập train khoảng 100.000 từ => one-hot vector sẽ có kích thước là 100.000 chiều! Số lượng lớn như vậy khiến quá trình huấn luyện rất chậm, đông thời không thể nạp hết vào GPU.<br>\n",
    "Có 2 cách để giảm bợt kích thước từ điển:\n",
    "> 1. Chỉ lấy n từ đầu tiên trong từ điển.\n",
    "> 2. Lấy m từ cuối cùng trong từ điển. <br>\n",
    "> Chúng ta sẽ sử dụng cách đầu tiên, giữ 25.000 từ đầu tiên trong vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Có một câu hỏi: Tại sao chúng ta chỉ xây dựng Vocab trên tập train? <br>\n",
    "=> Khi test, chúng ta không muốn mô hình biết trước các từ có trong tập test, như vậy tính khách quan của mô hình sẽ không còn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary:  25002\n",
      "Unique tokens in LABEL vocabulary:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique tokens in TEXT vocabulary: \", len(TEXT.vocab))\n",
    "print(\"Unique tokens in LABEL vocabulary: \", len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kích thước từ điển là 25002 thay vì 25000 là do có token đệm (pad). <br>\n",
    "Khi ta truyền các câu vào mô hình, ta sẽ truyền theo từng batch, tất cả các câu phải có cùng kích thước, các câu ***ngắn hơn câu dài nhất trong từ điển*** đều được đệm sao cho các thành phần trong batch có cùng kích thước. <br>\n",
    "Ví dụ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Padding example](./image/1_padsentence_example.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 232316),\n",
       " (',', 219807),\n",
       " ('.', 189207),\n",
       " ('and', 125132),\n",
       " ('a', 124608),\n",
       " ('of', 115066),\n",
       " ('to', 107184),\n",
       " ('is', 87276),\n",
       " ('in', 70016),\n",
       " ('I', 61807),\n",
       " ('it', 61373),\n",
       " ('that', 56356),\n",
       " ('\"', 50562),\n",
       " (\"'s\", 49467),\n",
       " ('this', 48446),\n",
       " ('-', 42132),\n",
       " ('/><br', 40779),\n",
       " ('was', 40081),\n",
       " ('as', 34777),\n",
       " ('with', 34057)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'neg': 0, 'pos': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bước cuối cùng trong việc chuẩn bị dữ liệu là tạo iterators. Iterators cho phép lặp xuyên suốt training/evaluation loop, và trả về batch (đã được chuyển về dạng tensor) ở mỗi lần lặp. <br>\n",
    "Chúng ta sẽ sử dụng ***BucketIterator***, _trả về một batch các sentences có cùng kích thước_, giúp **tối thiểu hóa việc pading sentences**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator,\\\n",
    "valid_iterator,\\\n",
    "test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),                \n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mô hình gồm 3 layer: _embedding_ layer, _RNN_ layer và _linear_ layer. Các layer có tham số được khởi tạo ngẫu nhiên nếu không được chỉ định ban đầu. <br>\n",
    "_Embedding_ layer được sử dụng để transform các one-hot vector ở dạng thưa thành các dense embedding vector (dense có nghĩa là vector có chiều nhỏ hơn và các phần tử là các số thực). Embedding layer thực chất là một lớp fully connected. Bên cạnh việc giảm chiều dữ liệu cho RNN, một số lý thuyết chỉ ra rằng các từ giống nhau trong phần nhận xét sẽ được ánh xạ gần nhau trong không gian dense vector. Đọc thêm về word embedding tại đây: [Here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/). <br>\n",
    "RNN layer tương tự như RNN mô tả phía trên. <br>\n",
    "Cuối cùng là lớp linear transform hidden state thành đầu ra có số chiều phù hợp thông qua một lớp fully connected f($h_T$). <br>\n",
    "Phương thức **forward** được gọi khi ta truyền vào một example point. <br>\n",
    "Mỗi một input batch được truyền qua một embedding layer để thu được **embedded**, là một dense vector biểu diễn câu ta truyền vào. **embedded** là một tensor có kích thước **[sentence length, batch size, embedding dim]**. <br>\n",
    "Sau đó **embedded** được truyền vào RNN layer. <br>\n",
    "RNN trả về 2 tensors:\n",
    "1. ***output*** có kích thước **[sentence length, batch size, embedding dim]**.\n",
    "2. ***hidden*** có kích thước **[1, batch size, embedding dim]**.\n",
    "> ***output*** là sự kết hợp của các hidden state qua các bước, trong khi ***hidden*** đơn giản là hidden state cuối cùng.\n",
    "3. Cuối cùng, ta truyền ***hidden*** vào lớp linear để sinh ra kết quả."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Padding example](./image/1_rnn_fc_model.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        # output = [sent len, batch size, hid dim]\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input_dim là kích thước của one-hot vectors, bằng vocabulary size.\n",
    "* embedding_dim là kích thước của dense word vectors. Thường nằm trong khoảng 50 - 250, tuy nhiên nó còn phụ thuộc vào kích thước vocabulary.\n",
    "* hidden_dim là kích thước của hidden states, thường nằm trong khoảng 100 - 500, tuy nhiên nó còn phụ thuộc vào kích thước vocabulary, kích thước của dense vector và độ phức tạp của task đang thực hiện.\n",
    "* output_dim là kích thước đầu ra, trong trường hợp này là 0 hoặc 1, 2 class biểu thị cho 2 nhãn (neg và pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "model_simple = RNN(input_dim, embedding_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_simple):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, ta cần tạo một optimizer. Đây bản chất là một thuật toán giúp cập nhật tham số cho mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_simple.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta cần định nghĩa hàm loss (trong pytorch thường gọi là criterion). <br>\n",
    "Do đầu ra là 0 hoặc 1, ta sử dụng Binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = model_simple.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Hàm ***train*** lặp xuyên suốt training set, mỗi một batch một lần. <br>\n",
    "***model.train()*** đưa mô hình vào chế độ huấn luyện - bật _drop out_ và _batch normalization_. <br>\n",
    "> * Với mỗi batch training, đầu tiên ta cần đưa gradients về 0. Mỗi tham số của mô hình có thuộc tính ***grad*** lưu trữ gradient được tính bởi ***creterion***. Pytorch không tự động đưa gradient về 0, do đó ta cần thực hiện. <br>\n",
    "> * Lưu ý, khi đưa một batch các câu, ***batch.text*** vào mô hình, ta không cần dùng phương thức ***forward***. ***squeeze*** cần thiết bởi việc dự đoán được khởi tạo với kích thước ***[batch size, 1]***, và ta cần xóa bỏ chiều 1, predictions yêu cầu đầu vào có kích thước ***[batch size]***. <br>\n",
    "> * Loss và accuracy được tính toán bằng cách sử dụng đầu ra của mô hình và label của batch - ***batch.label***, loss được tính trung bình trên toàn batch. <br>\n",
    "> * Ta tính gradient của mỗi tham số bằng ***loss.backward()***, sau đó cập nhật gradient bằng ***optimizer.step()***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * ***evaluate*** khá giống với ***train***, tuy nhiên ta cần điều chỉnh một chút.\n",
    "> * ***model.eval()*** đưa mô hình vào chế độ đánh giá - tắt dropout và batch normalization.\n",
    "> * Quá trình evaluate không cần tính đạo hàm => sử dụng ***with torch.no_grad()*** để giảm bộ nhớ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9060\\2602957732.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_simple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_simple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9060\\1778507431.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\longln3\\Anaconda3\\envs\\python3.7.6\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\longln3\\Anaconda3\\envs\\python3.7.6\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model_simple, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_simple, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_simple.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_simple.state_dict(), 'tut1-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.710 | Test Acc: 47.25%\n"
     ]
    }
   ],
   "source": [
    "model_simple.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model_simple, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Gói các câu đã được đệm lại cùng với nhau\n",
    "> * Sử dụng pre-train word embeddings.\n",
    "> * Sử dụng kiến trúc RNN khác.\n",
    "> * Sử dụng bidirectional RNN.\n",
    "> * Sử dụng RNN nhiều lớp (multi-layer RNN).\n",
    "> * Regularization.\n",
    "> * Sử dụng optimizer khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
