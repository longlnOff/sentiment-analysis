{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Multi-lcass Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở các notebooks trước, chúng ta đã được hiện phân tích cảm xúc trên bộ dữ liệu chỉ có 2 nhãn là: tích cực (pos) và tiêu cực (neg). Khi làm việc với bài toán 2 nhãn, đầu ra chỉ là 1 đại lượng vô hướng nằm trong khoảng [0, 1], biểu thị xác suất đối tượng thuộc class nào. khi có nhiều hơn 2 nhãn, đầu ra sẽ là một vector C chiều, với C là số lượng class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook này, chúng ta sẽ thực hiện phân loại trên bộ dữ liệu có 6 classes. Lưu ý bộ dữ liệu này không phải là bộ dữ liệu phân tích cảm xúc, đó là bộ dữ liệu về phân loại câu hỏi: **Phân loại xem câu hỏi thuộc loại nào?**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, ta thực hiện xây dựng và xử lý dữ liệu.\n",
    ">> Điểm khác biệt đầu tiên so với bài trước là: ta không cần thiết lập `dtype` trong trường `LABEL`. Khi thực hiện các bài toán multi-class, PyTorch mong muốn labels phải được số hóa, có dạng `LongTensor`s. <br>\n",
    ">> Điểm khác biệt thứ 2 là: ta sử dụng `TREC` để tải tập dữ liệu TREC. Đối số `fine_grained` sẽ kiểm soát số lượng nhãn của bài toán, nếu là `True`, ta sẽ sử dụng 50 classes, nếu là `False` ta sẽ sử dụng 6 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                    tokenizer_language='en_core_web_sm')\n",
    "\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cùng tìm hiểu về dữ liệu nào!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['What', 'is', 'a', 'Cartesian', 'Diver', '?'], 'label': 'DESC'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, chúng ta xây dựng vocab. Do dataset này khá nhỏ (xấp xỉ 3800 training examples) và có số lượng tokens trong vocab cũng khá nhỏ (xấp xỉ 7500 unique tokens) nên ta không cần thiết lập tham số `max_size` trong vocab như các ví dụ trước:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(   train_data,\n",
    "                    vectors = \"glove.6B.100d\",              # use pre-trained GloVe vectors\n",
    "                    unk_init = torch.Tensor.normal_)        # initialize unknown words with mean=0 and std=1 (phân phối chuẩn tắc\n",
    "                    \n",
    "                    \n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kiểm tra các labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khi ta sử dụng tập dữ liệu nhỏ, bài toán có 6 classes: <br>\n",
    ">> * `HUM` tương ứng với câu hỏi do con người đặt.\n",
    ">> * `ENTY` là câu hỏi về thực thể.\n",
    ">> * `DESC` là câu hỏi yêu cầu mô tả.\n",
    ">> * `NUM` là câu hỏi yêu cầu câu trả lời là số.\n",
    ">> * `LOC` là câu hỏi yêu cầu câu trả lời là vị trí.\n",
    ">> * `ABBR` là câu hỏi về từ viết tắt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'HUM': 0, 'ENTY': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta tạo iterators như mọi khi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                                                (train_data, valid_data, test_data),  # we pass in the datasets we want the iterator to draw data from\n",
    "                                                batch_size = BATCH_SIZE,              # batch size\n",
    "                                                sort_key=lambda x: len(x.text),       # sort by text length\n",
    "                                                sort_within_batch=True,\n",
    "                                                device = device)                      # on what device the iterator should load the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta sẽ sử dụng model CNN tương tự như notebook trước, tuy nhiên các model trong các notebook số 1, 2, 3 cũng có thể làm việc trên tập dữ liệu này. Điểm khác biệt là `output_dim` bây giờ là **C** thay vì 1 như trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNs(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1,\n",
    "                                                out_channels = n_filters,\n",
    "                                                kernel_size = (fs, embedding_dim)) for fs in filter_sizes\n",
    "                                    ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "            \n",
    "            #text = [sent len, batch size]\n",
    "    \n",
    "            text = text.permute(1, 0)\n",
    "    \n",
    "            #text = [batch size, sent len]\n",
    "    \n",
    "            embedded = self.embedding(text)\n",
    "    \n",
    "            #embedded = [batch size, sent len, emb dim]\n",
    "    \n",
    "            embedded = embedded.unsqueeze(1)\n",
    "    \n",
    "            #embedded = [batch size, 1, sent len, emb dim]\n",
    "    \n",
    "            conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "    \n",
    "            #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "    \n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "    \n",
    "            #pooled_n = [batch size, n_filters]\n",
    "    \n",
    "            cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "    \n",
    "            #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "    \n",
    "            return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta define model, ddamr baor rằng `OUPUT_DIM` = **C**. Ta có thể lấy **C** bằng các lấy kích thước của `LABEL` vocab, tương tự như dùng chiều dài của `TEXT` vocab để lấy kích thước từ điển đầu vào. <br>\n",
    "> Do dataset ta dùng ở bài này có kích thước nhỏ hơn IDBM nên ta dùng filters có kích thước nhỏ hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNNs(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kiểm tra số lượng tham số, ta sẽ thấy do dùng filters có kích thước nhỏ hơn nên số lượng tham số cũng ít hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 842,406 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
       "        ...,\n",
       "        [-0.3110, -0.3398,  1.0308,  ...,  0.5317,  0.2836, -0.0640],\n",
       "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
       "        [ 0.4306,  1.2011,  0.0873,  ...,  0.8817,  0.3722,  0.3458]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khởi tạo các từ không có trong từ điển của pre-trained vectors và padding tokens bằng tensor 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở các notebooks trước, do có 2 nhãn nên ta dùng `BCEWithLogitsLoss`. Tuy nhiên ở notebook này, bài toán là *multi-class classification* nên ta sử dụng `CrossEntropyloss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "critertion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = critertion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta sử dụng `argmax` để lấy được địa chỉ của giá trị lớn nhất của mỗi phần tử trong batch, sau đó ta đếm số lần giá trị nhãn dự đoán bằng giá trị nhãn thực tế, cuối cùng lấy trung bình của batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    correct = max_preds.eq(y.view_as(max_preds)).sum()\n",
    "    return correct.float() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quy trình training tương tự như các notebooks trước, có 1 chút khác biệt là ta cần `squeeze` dự đoán của model do `CrossEntropyloss` yêu cầu đầu vào có size **[batch size, n classes]** và label có size **[batch size]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý là label phải có dạng `LongTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.text)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hàm đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.084 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.427 |  Val. Acc: 85.13%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.065 | Train Acc: 98.67%\n",
      "\t Val. Loss: 0.428 |  Val. Acc: 85.58%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.85%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 85.12%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.045 | Train Acc: 99.19%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 86.00%\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.039 | Train Acc: 99.40%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 86.81%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.324 | Test Acc: 88.46%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/tut5-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hàm dự đoán tương tự như các notebooks trước, khác biệt nho nhỏ là ta sử dụng `argmax` để lấy địa chỉ của giá trị dự đoán cao nhất. Sau đó ta dùng địa chỉ này để chuyển về dạng người cho dễ nhận biết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += [''] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 0 = HUM\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_class(model, \"Who is Keyser Söze?\")\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 3 = NUM\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_class(model, \"How many minutes are in six hundred and eighteen hours?\")\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 4 = LOC\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_class(model, \"What continent is Bulgaria in?\")\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 5 = ABBR\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_class(model, \"What does WYSIWYG stand for?\")\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
