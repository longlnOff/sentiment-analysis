{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Updated Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở Notebook trước, chúng ta đã học về các kiến thức nền tảng của phân tích cảm xúc. Trong notebook lần này, chúng ta sẽ phân tích và cố gắng đạt được kết quả tốt hơn. <br>\n",
    "Dưới đây là danh sách công việc sẽ thực hiện trong notebook này: <br>\n",
    "* Gói, nhóm các chuỗi đã được đệm vào với nhau.\n",
    "* Sử dụng pre-trained word embedding.\n",
    "* Sử dụng cấu trúc RNN khác.\n",
    "* Sử dụng cấu trúc bidirectional RNN.\n",
    "* Sử dụng multi-layer RNN.\n",
    "* Áp dụng Regularization.\n",
    "* Sử dụng Optimizer khác. <br>\n",
    "\n",
    "***=> Sau khi thực hiện các phương pháp trên, chúng ta sẽ đạt được accuracy khoảng 84%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tương tự như ở notebook 1, chúng ta sẽ định nghĩa ***Fields*** và chia train/valid/test. <br>\n",
    "Chúng ta sẽ sử dụng *packed padded sequences* - thứ khiến mô hình RNN chỉ xử lý các phần tử *non-padded* trong chuỗi, bất cứ phần tử nào đã được padded, đầu ra sẽ là một zero tensor. Để sử dụng *packed padded sequences*, chúng ta cần cho RNN biết trước chiều dài của chuỗi thực. Để làm được việc đó, chúng ta cho ***include_lengths=True*** trong trường ***TEXT***. Sau khi làm như vậy, ***batch.text*** bây giờ là một tupple với phần tử đầu tiên là sentences của chúng ta (các tensor dạng số đã được đệm) và phần tử thứ 2 là chiều dài thực tế của sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                    tokenizer_language='en_core_web_sm',\n",
    "                    include_lengths = True)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tải dữ liệu IDMb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tạo validation từ tập train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, chúng ta sẽ sử dụng pre-trained word embeddings. Thay vì khởi tạo word embeddings một cách ngẫu nhiên, ta khởi tạo chúng bằng các vectors đã được huấn luyện trước. Chỉ cần set đối số ***build_vocab=tên vector***, Torchtext sẽ tự động tải và liên kết chúng với các từ trong từ điển của chúng ta. <br>\n",
    "> Ở đây, chúng ta sẽ sử dụng ***glove.6B.100d*** vectors. ***glove*** là một thuật toán dùng để tính toán các vectors, chi tiết xem thêm tại [đây](https://nlp.stanford.edu/projects/glove/). 6B ám chỉ có 6 tỉ tokens, 100d nghĩa là vector có 100 chiều. <br>\n",
    "> Theo lý thuyết, các pre-trained vectors mang hàm ý: các từ có nghĩa gần nhau sẽ nằm gần nhau trong không gian vector. Việc sử dụng pre-trained vector giúp cho embedding layer có giá trị khởi tạo tốt và không phải học mối quan hệ giữa các từ từ đầu. <br>\n",
    "> Lưu ý, Torchtext sẽ mặc định khởi tạo các từ có trong từ điển NHƯNG KHÔNG CÓ TRONG PRE-TRAINED EMBEDDING giá trị 0. Tuy nhiên chúng ta sẽ khởi tạo ngẫu nhiên chúng theo phân phối chuẩn Gaussian bằng cách sử dụng ***unk_init = torch.Tensor.normal_***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\",         # sử dụng pre-trained embedding\n",
    "                 unk_init = torch.Tensor.normal_)   # khởi tạo các từ không có trong pre-trained embedding theo phân phối chuẩn\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, chúng ta tạo iterator và sắp xếp các *packed padded sequences* theo độ dài của senteces bằng cách ***sort_within_batch = True***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "from torchtext.legacy import data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator,\\\n",
    "valid_iterator,\\\n",
    "test_iterator = \\\n",
    "    data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_within_batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lần này, các feature của model sẽ thay đổi đáng kể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different RNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Lần này, chúng ta sẽ sử dụng một kiến trúc RNN khác gọi là Long Short - Term Memory (LSTM). Kiến trúc RNNs tiêu chuẩn (như ở notebook 1) gặp phải một vấn đề khá nghiêm trọng là [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). Tuy nhiên, LSTM xử lý vấn đề này bằng cách tạo thêm một recurrent state được gọi là cell (c). Có thể hiểu cell như một memory trong LSTM - sử dụng nhiều gates để điều khiển luồng thông tin vào/ra memory. Nhấn vào [đây](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) để biết thêm chi tiết. Hiểu một cách đơn giản, LSTM là một hàm của $x_t, h_t, c_t$ thay vì $x_t, h_t$ như RNNs. <br>\n",
    "* ($h_t, c_t$) = LSTM($x_t, h_t, c_t$) <br>\n",
    "> Cấu trúc của LSTM như hình sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/2_lstm_figure1.PNG \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cell state được khởi tạo bằng tensor 0. Tuy nhiên, quá trình prediction chỉ sử dụng final hidden state, không sử dụng final cell state: $\\hat{y}$ = f($h_T$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khái niệm bidirectional RNN khá là đơn giản. Thay vì chỉ áp dụng RNN từ đầu tới cuối sentence (gọi là **forward RNN**) ta thực hiện thêm một bước là áp dụng RNN từ cuối cho tới đầu câu (gọi là **backward RNN**). Tại thời điểm t, **forward RNN** xử lý từ $x_t$, trong khi **backward RNN** xử lý từ $x_{T - t + 1}$. <br>\n",
    "> Trong pytorch, hidden state (và cell state) tensors được trả về bởi forward RNN và backward RNN được xếp chồng lên nhau. <br>\n",
    "> Ta dự đoán dựa trên last hidden state của forward RNN: $h_{T forward}$ và last hidden state của backward RNN: $h_{T backward}$. \n",
    "* $\\hat{y}$ = f($h_{T forward}$, $h_{T backward}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hình dưới mô tả cấu trúc bidirectional RNN với: màu xanh biểu thị backward, màu vàng biểu thị forward và linear layer được biểu thị bởi màu bạc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/2_bi-rnn_figure2.PNG \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-layer RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Multi-layer RNNs còn được gọi là deep RNNs. Ý tưởng ở đây là ta thêm các lớp RNNs bên trên lớp RNN tiêu chuẩn. Hidden state của lớp RNN phía dưới, tại thời điểm t sẽ là đầu vào của lớp RNN phía trên, cũng tại thời điểm t. Quá trình prediction được thực hiện ở *last hidden state* của *last layer* (layer cao nhất). <br>\n",
    "> Hình dưới minh họa kiến trúc multi-layer undirectional RNN, số layer là superscript (chỉ số trên). Lưu ý rằng mỗi layer cũng cần có giá trị khởi tạo hidden state ban đầu của riêng nó: ${h_0}^0$ và ${h_0}^1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/2_multilayer_rnn_figure3.PNG \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khi chúng ta thêm nhiều layer cũng như sử dụng bidirectional, mô hình của chúng ta có thêm nhiều parameters. Và, \"the more parameters you have in your model, the higher the probability that your model will overfit\". Để giải quyết vấn đề này, chúng ta sử dụng regularization, cụ thể là *dropout*. <br>\n",
    "> *Dropout* hoạt động bằng cách lựa chọn ngẫu nhiên một số neural trong layer và cho chúng bằng 0 trong quá trình lan truyền thuận. Số lượng neural bị drop là 1 hyperparameter. <br>\n",
    "> Có một lý thuyết chỉ ra rằng lý do dropout có thể xử lý vấn đề overfitting: Khi drop 1 số lượng neural nhất định, model sau khi bị dropout sẽ có số lượng parameters ít hơn (gọi là weaker model). Prediction của weaker model được lấy trung bình bên trong model. Do đó model gốc có thể được coi là sự kết hợp của các weaker models, và các weaker models này không bị overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta bổ sung thêm một nội dung khác cho mô hình này là: mô hình sẽ không học embedding đối với tokens. Lý do là việc padding tokens không liên quan gì tới việc xác định cảm xúc, quan điểm trong câu. Điều này có nghĩa *pad token* vẫn giữ nguyên giá trị khởi tạo của chúng. Ta thực hiện việc này bằng cách truyền thêm tham số ***padding_idx*** vào layer ***nn.Embedding***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta sẽ sử dụng LSTM thay cho RNN (`nn.LSTM` thay cho `nn.RNN`). Lưu ý là LSTM trả về một tupple chứa: ***output, final hidden state, final cell state*** trong khi RNN chỉ trả về ***output, final hidden state***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do final hidden state trong LSTM có cả forward và backward, đầu ra được ghép nối với nhau nên kích thước đầu vào của ***nn.Linear*** gấp 2 lần kích thước của hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Việc triển khai bidirectionality và thêm các layer khác được thực hiện bằng cách thiết lập 2 tham số: `num_layers` và `bidirectional` trong RNN/LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Triển khai dropout bằng ***nn.Dropout*** layer, áp dụng dropout sau mỗi layer mà ta muốn thực hiện dropout. <br>\n",
    "> **NOTE**: ***KHÔNG BAO GIỜ ÁP DỤNG DROPOUT CHO ĐẦU VÀO VÀ ĐẦU RA, CHỈ ÁP DỤNG DROPOUT CHO CÁC LỚP TRUNG GIAN***. <br>\n",
    "> LSTM có tham số `dropout` cho phép thêm dropout vào lớp kết nối giữa các hidden state trong một layer và hidden state trong layer kế tiếp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do áp dụng *packed padded sequences*, ta thêm đối số thứ 2 vào hàm ***foward*** là ***text_lengths***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trước khi truyền embeddings vào RNN, chúng ta cần gói chúng lại bằng cách sử dụng: ***nn.utils.rnn.packed_padded_sequence***. Việc này khiến RNN chỉ xử lý các non-padded elements trong sequence. RNN sẽ trả về ***packed_output***, ***hidden*** và ***cell*** states. Nếu không sử dụng packed padded sequences, ***hidden*** và ***cell*** là các tensor từ last element trong sequence - khả năng cao là pad token. Tuy nhiên khi sử dụng packed padded sequences, ***hidden*** và ***cell*** là tensors từ last non-padded element trong sequence. Lưu ý: ***lengths*** và ***packed_padded_sequence*** phải được chuyển về CPU tensor bằng cách `.to('cpu')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau đó, chúng ta cần unpack chuỗi đầu ra bằng ***nn.utils.rnn.pad_paced_sequence*** để biến đổi chúng từ dạng packed sequence sang tensor. Các phần tử trong ***output*** từ các padding tokens sẽ có giá trị tensor 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> final hidden state, ***hidden*** có kích thước: [**num layers * num directions, batch_size, hidden dim**]. Thứ tự của chúng sẽ là:  [**forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n**]. <br>\n",
    "> Chúng ta cần lấy final layer foward và final layer backward hidden states, ta sẽ lấy 2 lớp cuối cùng bằng 2 câu lệnh sau: ***hidden[-2,:,:]*** và ***hidden[-1,:,:]***, sau đó ghép nối chúng và truyền vào linear layer (sau khi áp dụng dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Khi truyền vào tham số padding_idx, embedding sẽ không tính toán gradient cho các từ có index bằng padding_idx\n",
    "        # ở đây là các pad tokens vì pad tokens không ảnh hưởng gì đến quá trình nhận định cảm xúc trong câu.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        # output = [sent len, batch size, hid dim * num directions]\n",
    "        # output over padding tokens are zero tensors\n",
    "        \n",
    "        # hidden = [num layers * num directions, batch size, hid dim]\n",
    "        # cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Để đảm bảo pre-trained vetors được load vào model, ***EMBEDDING_DIM*** phải bằng với số chiều của pre-trained vector. <br>\n",
    "> Ta lấy chỉ số của pad tokens trong từ điển bằng attribute ***pad_token***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "# We retrieve the embeddings from the field's vocab, and check they're the correct size, [vocab size, embedding dim]\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau đó, chúng ta sẽ thay thế weights của embedding layer bằng pre-trained embeddings <br>\n",
    "> NOTE: phải dùng ***weight.data***, KHÔNG phải ***weight***!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1386,  0.1180,  0.3534,  ...,  0.1226,  0.5973, -0.1702],\n",
       "        [-0.0786,  0.0541, -0.0993,  ...,  0.2565, -0.1874, -0.4428],\n",
       "        [-0.3617,  0.6201,  0.1105,  ...,  0.2994, -0.5920,  1.0949]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khởi tạo các token không có trong từ điển pre-trained và pad token có giá trị là tensor 0 do chúng không liên quan gì đến việc phân biệt cảm xúc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.1386,  0.1180,  0.3534,  ...,  0.1226,  0.5973, -0.1702],\n",
      "        [-0.0786,  0.0541, -0.0993,  ...,  0.2565, -0.1874, -0.4428],\n",
      "        [-0.3617,  0.6201,  0.1105,  ...,  0.2994, -0.5920,  1.0949]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thay vì sủ dụng SGD, notebook này sẽ sử dụng Adam. nhấn vào [đây](http://ruder.io/optimizing-gradient-descent/index.html) để hiểu thêm về thuật toán Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta cần định nghĩa hàm loss (trong pytorch thường gọi là criterion). <br>\n",
    "Do đầu ra là 0 hoặc 1, ta sử dụng Binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Hàm ***train*** lặp xuyên suốt training set, mỗi một batch một lần. <br>\n",
    "***model.train()*** đưa mô hình vào chế độ huấn luyện - bật _drop out_ và _batch normalization_. <br>\n",
    "> * Với mỗi batch training, đầu tiên ta cần đưa gradients về 0. Mỗi tham số của mô hình có thuộc tính ***grad*** lưu trữ gradient được tính bởi ***creterion***. Pytorch không tự động đưa gradient về 0, do đó ta cần thực hiện. <br>\n",
    "> * Lưu ý, khi đưa một batch các câu, ***batch.text*** vào mô hình, ta không cần dùng phương thức ***forward***. ***squeeze*** cần thiết bởi việc dự đoán được khởi tạo với kích thước ***[batch size, 1]***, và ta cần xóa bỏ chiều 1, predictions yêu cầu đầu vào có kích thước ***[batch size]***. <br>\n",
    "> * Loss và accuracy được tính toán bằng cách sử dụng đầu ra của mô hình và label của batch - ***batch.label***, loss được tính trung bình trên toàn batch. <br>\n",
    "> * Ta tính gradient của mỗi tham số bằng ***loss.backward()***, sau đó cập nhật gradient bằng ***optimizer.step()***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Khi set ***include_lengths=True***, ***batch.text*** là một tupple với phần tử đầu tiên là một tensor, phần tử thứ 2 là chiều dài thực của sequence. Ta chia chúng vào 2 biến text và text_lengths trước khi truyền chúng vào model. <br>\n",
    "> * **NOTE**: Nhớ dùng model.train() để bật dropout trong lúc train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fnially, we train model =))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hàm ***predict_sentiment*** thực hiện các công việc sau:\n",
    "* Đưa model vào chế độ evaluation.\n",
    "* tokenizes các sentence.\n",
    "* Đánh chỉ số tokens bằng cách chuyển chúng thành các số biểu diễn chúng trong vocab.\n",
    "* Lấy length của sequence.\n",
    "* Chuyển chỉ số từ Python list sang Pytorch tensor.\n",
    "* Thêm 1 chiều batch với `unsqueeze`.\n",
    "* Chuyển length về dạng tensor.\n",
    "* Chuyển đầu ra về dạng nhị phân với `sigmoid`.\n",
    "* Chuyển tensor chứa 1 giá trị đơn lẻ về dạng integer với phương thức `item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook tiếp theo, chúng ta sẽ xây dựng một mô hình có độ chính xác tốt hơn và thời gian huấn luyện nhanh hơn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
