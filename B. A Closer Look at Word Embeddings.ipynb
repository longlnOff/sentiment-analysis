{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - A Closer Look at Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở các notebooks trước, chúng ta đã biết và làm việc với word embeddings (hay còn gọi là word vectors) khá nhiều. Trong notebook này, chúng ta sẽ cùng đào sâu hơn về word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Embeddings biến đổi một one-hot vector đã được mã hóa thành một vector số thực có dimension nhỏ hơn khá nhiều. One-hot vector còn được gọi là *sparse vector* - vector thưa, còn vector số thực được gọi là *dense vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ý tưởng chính trong word embeddings là các từ có **bối cảnh - context** xuất hiện giống nhau thì sẽ **gần** nhau hơn trong vector space. Ví dụ: 2 từ nằm gần nhau trong vector space tức là khoảng cách Euclidean giữa chúng nhỏ. **Bối cảnh - context** ở đây được hiểu là **các từ đi kèm với từ mà ta đang quan tâm**. Ví dụ trong câu: \"I purchased some items at the shop\" và câu \"I purchased some items at the store\", từ \"shop\" và \"store\" xuất hiện trong cùng một bối cảnh nên chúng nằm gần nhau trong vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *word2vec* là một lớp các thuật toán tính toán word vectors từ corpus. Nếu muốn tìm hiểu thêm về *word2vec*, nhần vào [đây](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) và [đây](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/). Trong notebook này, chúng ta sẽ sử dụng *GloVe* vectors, *GloVe* là một thuật toán khác để tính word vectors. Nhấn vào [đây](https://nlp.stanford.edu/projects/glove/) để tìm hiểu thêm về *GloVe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong PyTorch, ta sử dụng word vectors với `nn.embedding` layer. Layer này nhận đầu vào là tensor có size: **[sentence length, batch size]** và trả về tensor có size: **[sentence length, batch size, embedding dimensions]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Từ notebooks số 2 trở đi, ta đều sử dụng pre-trained word embeddings (cụ thể là GloVe vectors) do TorchText cung cấp. Embeddings này đã được huấn luyện trên kho dữ liệu khổng lồ. Ta có thể sử dụng pre-trained vectors này bên trong bất cứ model nào mà ta muốn. Ý tưởng khi áp dụng pre-trained word embedding là models sẽ học được context của mỗi từ, cho ta điểm xuất phát tốt hơn cho models của mình, dẫn đến việc training nhanh hơn và cho kết quả tốt hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook này, chúng ta sẽ không training models nữa, thay vào đó ta sẽ nghiên cứu về word embeddings và đi tìm một vài sự thật thú vị về chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Phần lớn code trong này được tham khảo từ [đây](https://github.com/spro/practical-pytorch/blob/master/glove-word-vectors/glove-word-vectors.ipynb). Nếu bạn muốn đọc thêm về word embeddings, nhấn vào [đây](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, ta load GloVe vectors. Trường `name` xác định vectors nào được train, ở đây `6B` tức corpus có 6 tỷ words. Đối số `dim` xác định số chiều của word vectors. GloVe vector có số chiều nhận các giá trị: 50, 100, 200, 300. Bên cạnh đó còn có `42B` và `840B` vectors, tuy nhiên chúng mặc định có dimension là 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "print('Loaded {} words'.format(len(glove.itos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GloVe vocabulary có 400000 unique words. <br>\n",
    "> **Trong tập GloVe vectors, tất cả các từ đều ở dạng lower-case**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `glove.vectors` thực chất laf1 tensor chứa các giá trị embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 100])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta có thể kiểm tra các word liên kết với một hàng bằng cách kiểm tra `itos` (int to string) list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.itos[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Như ở trên, từ \"the\" liên quan tới vector hàng thứ 0, từ \",\" liên quan tới vector hàng thứ 1, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta cũng có thể làm ngược lại, sử dụng `stoi` (string to int) để tìm hàng liên quan tới từ. Lưu ý, nếu từ ta tìm kiếm không có trong vocabulary, torchtext sẽ thông báo lỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta có thể lấy biểu diễn vector của một từ bằng cách sau:\n",
    ">> 1. Đầu tiên lấy chỉ số của từ đó.\n",
    ">> 2. Dùng chỉ số đó để truy xuất vào hàng tương ứng trong word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = glove.stoi[\"the\"]\n",
    "glove.vectors[index].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta sẽ viết 1 hàm thực hiện việc lấy embeddings của từ. Hàm này nhận vào từ và trả về vector liên quan, nếu không có từ trong vocabulary, nó sẽ trả về lỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(embeddings, word):\n",
    "    assert word in embeddings.stoi, f'*{word}* not in vocabulary'\n",
    "    index = embeddings.stoi[word]\n",
    "    return embeddings.vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector(glove, \"q\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bây giờ, chúng ta sẽ cùng nghiên cứu về context của các từ khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nếu chúng ta muốn tìm các từ tương tự một từ cho trước, đầu tiên ta cần tìm biểu diễn vector của từ cho trước đó. Tiếp theo, ta quét khắp vocabulary để tính khoảng cách giữa từ trong từ điển và từ cho trước. Sau khi có được khoảng cách, ta sắp xếp theo thứ tự từ gần đến xa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hàm dưới đây trả về 5 gần nhất đối với từ đầu vào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def closest_words(embeddings, vector, n = 5):\n",
    "\n",
    "    distance = [(word, torch.dist(vector, get_vector(embeddings, word))) for word in embeddings.itos]\n",
    "\n",
    "    return sorted(distance, key = lambda w: w[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', tensor(0.)),\n",
       " ('prince', tensor(4.0922)),\n",
       " ('queen', tensor(4.2813)),\n",
       " ('monarch', tensor(4.4742)),\n",
       " ('brother', tensor(4.5367))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = get_vector(glove, \"king\")\n",
    "\n",
    "closest_words(glove, word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta tạo hàm `print_tuples` để hiển thị tuples được trả về từ `closest_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tuples(tuples):\n",
    "    for w, d in tuples:\n",
    "        print(f'({d:02.04f}) {w}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0000) king\n",
      "(4.0922) prince\n",
      "(4.2813) queen\n",
      "(4.4742) monarch\n",
      "(4.5367) brother\n"
     ]
    }
   ],
   "source": [
    "word_vector = get_vector(glove, \"king\")\n",
    "print_tuples(closest_words(glove, word_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một đặc tính khác của word embeddings là chúng có thể hoạt động giống các standard vector và cho ra kết quả thú vị."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(embeddings, word1, word2, word3, n=5):\n",
    "    \n",
    "    #get vectors for each word\n",
    "    word1_vector = get_vector(embeddings, word1)\n",
    "    word2_vector = get_vector(embeddings, word2)\n",
    "    word3_vector = get_vector(embeddings, word3)\n",
    "    \n",
    "    #calculate analogy vector\n",
    "    analogy_vector = word2_vector - word1_vector + word3_vector\n",
    "    \n",
    "    #find closest words to analogy vector\n",
    "    candidate_words = closest_words(embeddings, analogy_vector, n+3)\n",
    "    \n",
    "    #filter out words already in analogy\n",
    "    candidate_words = [(word, dist) for (word, dist) in candidate_words \n",
    "                       if word not in [word1, word2, word3]][:n]\n",
    "    \n",
    "    print(f'{word1} is to {word2} as {word3} is to...')\n",
    "    \n",
    "    return candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to king as woman is to...\n",
      "(4.0811) queen\n",
      "(4.6429) monarch\n",
      "(4.9055) throne\n",
      "(4.9216) elizabeth\n",
      "(4.9811) prince\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'man', 'king', 'woman'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đây là một ví dụ kinh điển, cho thấy tính chất của word embeddings. Tại sao vector 'woman' + vector 'king' - vector 'man' = vector 'queen'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta có thể hiểu như sau: vector 'king' - vector 'man' = vector 'hoàng gia'. Vector 'hoàng gia' này khi liên kết với vector 'man' sẽ cho ra người đàn ông trong hoàng gia (trong trường hợp này chính là 'king'). Tương tự nếu ta cộng vector 'hoàng gia' này với vector 'woman', ta sẽ có vector liên kết tương đương là vector 'queen'. <br>\n",
    "> Dưới đây là 1 ví dụ minh họa tương tự:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to actor as woman is to...\n",
      "(2.8133) actress\n",
      "(5.0039) comedian\n",
      "(5.1399) actresses\n",
      "(5.2773) starred\n",
      "(5.3085) screenwriter\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'man', 'actor', 'woman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat is to kitten as dog is to...\n",
      "(3.8146) puppy\n",
      "(4.2944) rottweiler\n",
      "(4.5888) puppies\n",
      "(4.6086) pooch\n",
      "(4.6520) pug\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'cat', 'kitten', 'dog'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france is to paris as england is to...\n",
      "(4.1426) london\n",
      "(4.4938) melbourne\n",
      "(4.7087) sydney\n",
      "(4.7630) perth\n",
      "(4.7952) birmingham\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'france', 'paris', 'england'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elvis is to rock as eminem is to...\n",
      "(5.6597) rap\n",
      "(6.2057) rappers\n",
      "(6.2161) rapper\n",
      "(6.2444) punk\n",
      "(6.2690) hop\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'elvis', 'rock', 'eminem'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beer is to barley as wine is to...\n",
      "(5.6021) grape\n",
      "(5.6760) beans\n",
      "(5.8174) grapes\n",
      "(5.9035) lentils\n",
      "(5.9454) figs\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'beer', 'barley', 'wine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Spelling Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một trong những tính chất thú vị nữa của word embeddings là chúng có thể sử dụng trong việc sửa lỗi chính tả!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Để tìm hiểu chi tiết hơn, click vào [đây](https://forums.fast.ai/t/nlp-any-libraries-dictionaries-out-there-for-fixing-common-spelling-errors/16411) và [đây](https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.heyday.xyz%2Fa-simple-spell-checker-built-from-word-vectors-9f28452b6f26)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, ta cần tập vocabulary lớn hơn của GloVe vectors, do lỗi chính tả không xuất hiện trong các vocabulary nhỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: vectors có kích thước khá lớn (~2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.840B.300d.zip: 2.18GB [10:03, 3.61MB/s]                                \n",
      "100%|█████████▉| 2196016/2196017 [05:47<00:00, 6313.76it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = torchtext.vocab.GloVe(name = '840B', dim = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kiểm tra kích thước vocabulary, ta thấy nó có hơn 2 triệu unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2196017, 300])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do vectors được huấn luyện trên tập vocabulary rất lớn và trong corpus cũng rất lớn nên các từ xuất hiện có khá ít sự khác nhau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0000) korea\n",
      "(3.9857) taiwan\n",
      "(4.4022) korean\n",
      "(4.9016) asia\n",
      "(4.9593) japan\n"
     ]
    }
   ],
   "source": [
    "word_vector = get_vector(glove, 'korea')\n",
    "\n",
    "print_tuples(closest_words(glove, word_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Muốn sửa lỗi chính tả, đầu tiên ta cần tìm vector cho từ bị lỗi: 'reliable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0000) relieable\n",
      "(5.0366) relyable\n",
      "(5.2610) realible\n",
      "(5.4719) realiable\n",
      "(5.5402) relable\n"
     ]
    }
   ],
   "source": [
    "word_vector = get_vector(glove, 'relieable')\n",
    "\n",
    "print_tuples(closest_words(glove, word_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chú ý, các viết đúng \"reliable\" không xuất hiện trong danh sách 10 từ gần nhất. Đáng nhẽ nó phải xuất hiện mới đúng?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Giả thuyết đưa ra là: các từ sai chính tả thường xuất hiện trong các văn bản không quan trọng như:tweet, comment, etc. Trong khi các từ đúng chính tả thường xuất hiện trong các văn bản quan trọng, yêu cầu kiểm duyệt nghiêm ngặt. Từ đó, chúng không có cùng context nên không xuất hiện trong danh sách 10 từ gần nhất cũng là dễ hiểu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thay vì chỉ sử dụng một example để tạo vector, ta sẽ sử dụng trung bình của nhiều examples và hy vọng kết quả sẽ tốt hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_vector = get_vector(glove, 'reliable')\n",
    "\n",
    "reliable_misspellings = ['relieable', 'relyable', 'realible', 'realiable', \n",
    "                         'relable', 'relaible', 'reliabe', 'relaiable']\n",
    "\n",
    "diff_reliable = [(reliable_vector - get_vector(glove, s)).unsqueeze(0) \n",
    "                 for s in reliable_misspellings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Trước tiên, tạo một vector đúng chính tả \"reliable\", sau đó tính độ sai khác giữa vector đúng chính tả và 8 vector sai chính tả. Tiếp theo ta ghép 8 \"miss spelling tensors\" này lại và unsqueeze batch dimension cho chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp đến, ta tính trung bình sai khác giữa 8 vector sai và vector đúng để tạo ra vector sai của riêng mình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelling_vector = torch.cat(diff_reliable, dim = 0).mean(dim = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cuối cùng, ta sửa lỗi bằng cách tìm từ gần nhất đối với tổng của 'vector đã bị sai chính tả ban đầu và vector sai chính tả ta tự tạo ra'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.1090) because\n",
      "(6.4250) even\n",
      "(6.4358) fact\n",
      "(6.4914) sure\n",
      "(6.5094) though\n"
     ]
    }
   ],
   "source": [
    "# vector sai chính tả ban đầu\n",
    "word_vector = get_vector(glove, 'becuase')\n",
    "\n",
    "# vector sai chính tả ta tự tạo ra\n",
    "misspelling_vector = torch.cat(diff_reliable, dim = 0).mean(dim = 0)\n",
    "\n",
    "print_tuples(closest_words(glove, word_vector + misspelling_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.4070) definitely\n",
      "(5.5643) certainly\n",
      "(5.7192) sure\n",
      "(5.8152) well\n",
      "(5.8588) always\n"
     ]
    }
   ],
   "source": [
    "word_vector = get_vector(glove, 'defintiely')\n",
    "\n",
    "print_tuples(closest_words(glove, word_vector + misspelling_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.9641) consistent\n",
      "(6.3674) reliable\n",
      "(7.0195) consistant\n",
      "(7.0299) consistently\n",
      "(7.1605) accurate\n"
     ]
    }
   ],
   "source": [
    "word_vector = get_vector(glove, 'consistant')\n",
    "\n",
    "print_tuples(closest_words(glove, word_vector + misspelling_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.6117) package\n",
      "(6.9315) packages\n",
      "(7.0195) pakage\n",
      "(7.0911) comes\n",
      "(7.1241) provide\n"
     ]
    }
   ],
   "source": [
    "word_vector = get_vector(glove, 'pakage')\n",
    "\n",
    "print_tuples(closest_words(glove, word_vector + misspelling_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nếu muốn tìm hiểu sâu hơn về sửa lỗi chính tả, nhấn vào [đây](https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.heyday.xyz%2Fa-simple-spell-checker-built-from-word-vectors-9f28452b6f26)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
