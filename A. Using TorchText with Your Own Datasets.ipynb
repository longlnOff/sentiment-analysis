{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Using TorchText with Your Own Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong Series này, chúng ta đa sử dụng tập dữ liệu IMDb trong TorchText. TorchText có rất nhiều tập dữ liệu kinh điển, phục vụ cho các bài toán như: classification, language modelling, sequence tagging, etc. Tuy nhiên, chúng ta sẽ cần phải sử dụng các dataset không nằm trong TorchText. May mắn là TorchText có các hàm support việc xây dựng dataset từ bộ dữ liệu bên ngoài."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Có 3 dạng data mà TorchText có thể đọc: `json`, `tsv` và `csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theo ý kiến của tôi, `json` là kiểu dữ liệu phù hợp nhất với TorchText**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bắt đầu với định dạng `json`. Hiển nhiên, data cần xử lý phải ở dạng `json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta define các trường sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "NAME = data.Field()\n",
    "PLACE = data.Field()\n",
    "SAYING = data.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta cần thông báo cho TorchText trường nào áp dụng cho `json` object nào."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đối với `json` data, ta cần tạo một dictionary đáp ứng các tiêu chí sau:\n",
    ">> * Key của dictionary phải match với key của `json` object.\n",
    ">> * Value của dictionary là một tupple với:\n",
    ">>> * Phần tử đầu tiên là tên của batch object's attribute.\n",
    ">>> * Phần tử thứ 2 là tên của trường `Field`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lý giải cho cụm từ \"batch objet's attribute\": Quay lại các notebooks trước, ta truy cập vào các trường `TEXT` và `LABEL` trong quá trình train/evaluation bằng cách sử dụng `batch.text` và `batch.label`. Ta truy cập được vào do TorchText thiết lập batch object có attribute `text` và `label`, mỗi attribute là một tensor chứa text và label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dưới đây là 1 số chú ý quan trọng:\n",
    ">> * Thứ tự của key trong dictionary `fields` không quan trọng, miễn là keys phải match với keys trong `json` data.\n",
    ">> * Tên của `Field` không nhất thiết phải match với keys trong `json` object. Ví dụ ta có thể sử dụng `PLACE` cho trường `location`.\n",
    ">> * Khi làm việc với `json` data, không phải lúc nào ta cũng sử dụng hết các keys có trong data.\n",
    ">> * Nếu values của `json` là string, `Fields` tokenization sẽ được áp dụng lên nó (mặc định sẽ là ngăn cách bởi spaces). Tuy nhiên, nếu values là một list, TorchText sẽ không áp dụng tokenization lên nó. Thông thường, ta nên tokenize data trước vì nó giúp tiết kiệm thời gian do TorchText không cần tokenize nữa.\n",
    ">> * Value trong các trường của `json` không nhất thiết phải có cùng kiểu. \n",
    ">> * Nếu sử dụng trường trong `json`, tất cả các single example phải là instance của trường đó. Trong ví dụ dưới, tất cả các examples phải có name, location và quote. Tuy nhiên, do ta không dùng trường age nên nếu examples thiếu trường age thì cũng không ảnh hưởng gì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'name': ('n', NAME), 'location': ('p', PLACE), 'quote': ('s', SAYING)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bây giờ, trong training loop, ta có thể lặp trong iterator và truy cập tên thông qua `batch.n`, location thông qua `batch.p` và quote thông qua `batch.s`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau đó, ta tự tạo datasets (`train_data` và `test_data`) bằng hàm `TabularDataset.splits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path = './data',\n",
    "    train = 'train.json',\n",
    "    test = 'test.json',\n",
    "    format = 'json',\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đối số `path` xác định thư mục cha của 2 file data, `train` và `test` arguments là đối số chứa tên của 2 file. ở đây train dataset có đường dẫn: `data/train.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở đây, ta đã thông báo cho function biết đang sử dụng `json` data, và truyền `fields` dictionary đã được định nghĩa từ trước."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dưới đây là ví dụ khi ta có sẵn 3 file: train.json, test.json và valid.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, valid_data = data.TabularDataset.splits(\n",
    "    path = './data',\n",
    "    train = 'train.json',\n",
    "    test = 'test.json',\n",
    "    validation = 'valid.json',\n",
    "    format = 'json',\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý, ở đây các tên trường (field names) là `n`, `p` và `s` khớp với những gì đã được define trong `fields` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bên cạnh đó, ta cũng cần chú ý: Từ \"United Kingdom\" trong `p` được tokenize trong khi \"united kingdom\" trong `s` không được tokenize. Lý do là TorchText sẽ không áp dụng tokenize khi value có kiểu dữ liệu là list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': ['John'],\n",
       " 'p': ['United', 'Kingdom'],\n",
       " 's': ['i', 'love', 'the', 'united kingdom']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Từ bây giờ, ta có thể sử dụng `train_data`, `test_data` và `valid_data` để xây dựng vocabulary và tạo iterators. Ta có thể truy cập vào các attribute bằng cách sử dụng `batch.n`, `batch.p` và `batch.s` để lấy names, places và sayings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV/TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tất nhiên, dữ liệu phải ở dạng CSV hoặc TSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Đối với CSV/TSV data, ta sử dụng list of tupple thay vì list như ở `json`. Phần tử đầu tiên trong tupple là tên của batch object's attribute, phần tử thứ hai là tên `Field`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Không giống như `json` data, tupples phải có thứ tự giống trong file `csv/tsv`. do đó, khi cần bỏ qua một cột data, ta sử dụng `None`. Tuy nhiên, nếu ta muốn sử dụng các cột dữ liệu liền nhau và đứng đầu tiên, ta chỉ cần dùng 2 tupples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('n', NAME), ('p', PLACE), (None, None), ('s', SAYING)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta chuyển `format` thành `tsv`, set tham số `skip_header = True` nếu data có header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, valid_data = data.TabularDataset.splits(\n",
    "    path = './data',\n",
    "    train = 'train.csv',\n",
    "    test = 'test.csv',\n",
    "    validation = 'valid.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': ['John'],\n",
       " 'p': ['United', 'Kingdom'],\n",
       " 's': ['i', 'love', 'the', 'united', 'kingdom']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tương tự với TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, valid_data = data.TabularDataset.splits(\n",
    "    path = './data',\n",
    "    train = 'train.tsv',\n",
    "    test = 'test.tsv',\n",
    "    validation = 'valid.tsv',\n",
    "    format = 'tsv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': ['John'],\n",
       " 'p': ['United', 'Kingdom'],\n",
       " 's': ['i', 'love', 'the', 'united', 'kingdom']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why JSON over CSV/TSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `csv` hoặc `tsv` data không thể lưu trữ trong lists. Điều đó có nghĩa là data không thể tokenize trước khi đưa vào TorchText, dẫn đến việc TorchText phải tokenize mỗi khi đưa data vào. Việc sử dụng các tokenizer bên ngoài như của `spaCy` tốn khá ít thời gian, do đó, **ta nên tokenize datasets và lưu chúng dưới dạng `json lines`**.\n",
    "2. Nếu tabs xuất hiện trong `tsv` data, còn commas xuất hiện trong `csv` data, TorchText sẽ nghĩ đó là ngăn cách cột, dẫn đến việc phân tích data bị sai. Do `json` data nằm trong dictionary, ta truy cập chúng thông qua keys nên không cần lo lắng về việc xuất hiện các ký tự ngăn cách. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xây dựng vocabulary cho datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME.build_vocab(train_data)\n",
    "PLACE.build_vocab(train_data)\n",
    "SAYING.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta cần tạo iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mặc định, train data được shuffled ở mỗi epoch, còn validation/test data thì được sắp xếp. Tuy nhiên TorchText không biết sắp xếp data nếu ta không yêu cầu nó làm, từ đó có thể gây lỗi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Có 2 cách để xử lý:\n",
    ">> * Truyền `sort = False`.\n",
    ">> * Sử dụng `sort_key`. `sort_key` nhận vào một hàm trả về key mà ta muốn sắp xếp dữ lệu theo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.n]:[torch.LongTensor of size 1x1]\n",
      "\t[.p]:[torch.LongTensor of size 2x1]\n",
      "\t[.s]:[torch.LongTensor of size 5x1]\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.n]:[torch.LongTensor of size 1x1]\n",
      "\t[.p]:[torch.LongTensor of size 2x1]\n",
      "\t[.s]:[torch.LongTensor of size 4x1]\n",
      "Valid:\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.n]:[torch.LongTensor of size 1x1]\n",
      "\t[.p]:[torch.LongTensor of size 1x1]\n",
      "\t[.s]:[torch.LongTensor of size 3x1]\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.n]:[torch.LongTensor of size 1x1]\n",
      "\t[.p]:[torch.LongTensor of size 2x1]\n",
      "\t[.s]:[torch.LongTensor of size 3x1]\n",
      "Test:\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.n]:[torch.LongTensor of size 1x1]\n",
      "\t[.p]:[torch.LongTensor of size 1x1]\n",
      "\t[.s]:[torch.LongTensor of size 2x1]\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.n]:[torch.LongTensor of size 1x1]\n",
      "\t[.p]:[torch.LongTensor of size 1x1]\n",
      "\t[.s]:[torch.LongTensor of size 4x1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort = False, #don't sort test/validation data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key = lambda x: x.s, #sort by s attribute (quote)\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "print('Train:')\n",
    "for batch in train_iterator:\n",
    "    print(batch)\n",
    "    \n",
    "print('Valid:')\n",
    "for batch in valid_iterator:\n",
    "    print(batch)\n",
    "    \n",
    "print('Test:')\n",
    "for batch in test_iterator:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
