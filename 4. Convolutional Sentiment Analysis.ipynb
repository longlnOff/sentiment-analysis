{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Convolutional Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở notebooks trước, chúng ta đã đạt được accuracy xấp xỉ 85% bằng cách sử dụng RNNs và áp dụng mô hình trong paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf). Trong notebook này, chúng ta sẽ cùng nhau tìm hiểu và áp dụng *convolutional neural network (CNN) để xây dựng một mô hình phân tích cảm xúc, áp dụng mô hình trong bài báo [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LƯU Ý: Notebook này chỉ tóm gọn về convolutional neural network. Để biết thêm chi tiết về CNNs, bạn có thể tham khảo [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/). và [Convolutional Neural Networks Coursera](https://www.coursera.org/learn/convolutional-neural-networks), hoặc [Convolutional Neural Networks blog](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thông thường, CNNs được sử dụng trong phân tích ảnh. CNNs thường được tạo bởi một hoặc nhiều lớp tích chập - *convolutional* layers, theo sau là một hoặc nhiều *linear layers*. *Convolutional* layers sử dụng các *filters* (còn được gọi là *kernels* hoặc *receptive fields*) quét ngang qua một bức ảnh và sinh ra một bức ảnh mới đã được xử lý. Bức ảnh mới này có thể đưa qua các lớp covolutional hoặc linear tùy bài toán. Mỗi filter đều có kích thước, ví dụ một filter 3x3 có chiều dài là 3, rộng 3 và tổng cộng có 9 trọng số. Trong xử lý ảnh truyền thống, các trọng số này sẽ được tính toán bằng tay. Tuy nhiên, với CNNs, các trọng số này được học thông qua *backpropagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ý tưởng của việc học các trọng số là: mỗi *convolutional* layer hoạt động như một bộ trích xuất đặc trưng - *feature extractors*, trích xuất những phần quan trọng trong bức ảnh. Ví dụ, nếu sử dụng CNN để phát hiện khuôn mặt, CNN sẽ nhìn vào những đặc trưng quan trọng của khuôn mặt như: mắt, mũi, miệng, lông mày, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thế, tại sao lại sử dụng CNNs cho văn bản? Tương tự như việc xử lý ảnh với filter 3x3, một filter 1x2 có thể nhanh chóng kiểm tra 2 từ liền nhau trong một đoạn văn bản, ví dụ: bi-grams. Ở Notebook trước, chúng ta sử dụng FastText model - sử dụng bi-grams bằng cách chèn chúng vào cuối mỗi câu. Lần này, chúng ta sẽ sử dụng CNNs để học bi-grams (1x2 filter) một cách tự động, bên cạnh đó còn học tri-grams (1x3 filter), ..., n-grams (1xn filter) trong văn bản. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trực giác cho thấy rằng: nếu có một số bi-grams, tri-grams và n-grams nhất định xuất hiện trong bài đánh giá, đó sẽ là dấu hiệu tốt để đưa ra kết luận cuối cùng cho bài đánh giá là tích cực (pos) hay tiêu cực (neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta xây dựng tập dữ liệu tương tự notebook 2. <br>\n",
    "> Do convolutional layers yêu cầu số chiều của batch phải nằm trước nên ta sử dụng `batch_first=True` khi khởi tạo `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                    tokenizer_language='en_core_web_sm',\n",
    "                    batch_first = True)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xây dựng vocab và load pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                    max_size = MAX_VOCAB_SIZE,\n",
    "                    vectors = \"glove.6B.100d\",\n",
    "                    unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tương tự như các notebook trước, ta tạo iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = \\\n",
    "    data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, chúng ta cần phải hình dùng cách áp dụng CNNs cho text. Trong trường hợp ảnh xám, mỗi bức ảnh sẽ có 2 chiều (ảnh màu sẽ có nhiều dimensional hơn ảnh xám) trong khi đó, text có 1 dimensional. Tuy nhiên, ta đều biết rằng bước đầu tiên trong các NLP pipelines là biến đổi các từ thành word embeddings. Đây là cách ta biểu diễn các words ở dạng 2 dimensions, các từ nằm theo 1 trục, trục còn lại là các phần tử của word embeddings. Vì vậy, ta có thể coi word embeddings như là một bức ảnh xám. Quan sát word embeddings biểu diễn 1 câu dưới đây: <br>\n",
    ">> I hate this film. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/4_example_word_embeddings.PNG \"LSTM\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta có thể sử dụng filter có kích thước **[n x emb_dim]**. Filter này sẽ bao quát hoàn toàn n chuỗi từ, do chiều rộng của filter là `emb_dim`. Trong bức ảnh bên dưới, word vectors được biểu diễn bởi màu xanh, ở đây ta có 4 từ, mỗi từ có 5 dimensional embeddings, tạo nên 1 tensor 2 chiều có kích thước **[4x5]**. Giả sử ta muốn tạo 1 filter sẽ học 2 từ 1 lúc (bi-grams) nên kích thước của nó sẽ là **[2x5]** (filter có màu vàng) và mỗi phần tử trong filter có một *weight* tương ứng (trong trường hợp này là 10 *weights* do filter có size 2x5). Đầu ra của 1 filter sẽ là 1 số thực, được tính bằng cách lấy tổng có trọng số của tất cả các phần tử được quét bởi filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/4_filter_figure1.PNG \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau đó, filter di chuyển xuống dưới dọc theo câu để kiểm tra bi-gram tiếp theo và sinh ra một đầu ra mới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/4_filter_figure2.PNG \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cuối cùng, filter di chuyển xuống cuối và ta thu được đầu ra của bi-gram cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/4_filter_figure3.PNG \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong trường hợp này (và các trường hợp mà chiều rộng của filter bằng với chiều rộng của \"ảnh\" - tức kích thước của 1 embedding vector), đầu ra sẽ là một vector có số phần tử bằng \"chiều cao của ảnh\" - trong trường hợp này là chiều dài của câu trừ đi chiều cao của filter và cộng thêm một:\n",
    ">> kích thước đầu ra = chiều dài của câu - chiều cao của filter + 1\n",
    ">>> trong trường hợp này sẽ là: 4 - 2 + 1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Các model có CNNs đều có rất nhiều filter. Ý tưởng ở đây là **mỗi filter sẽ học một thuộc tính khác nhau để trích xuất**. Ở ví dụ trên, với filter có kích thước **[2 x emb_dim]** sẽ học được sẽ xuất hiện của mỗi cặp từ (hay còn gọi là bi-grams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model chúng ta sẽ xây dựng còn sử dụng các filter có kích thước khác nhau, chiều cao lần lượt là 3, 4 và 5, mỗi kích thước có 100 filters. Trực giác cho thấy, sự xuất hiện của các tri-grams, 4-grams và 5-grams nhất định sẽ ảnh hưởng đến kết quả dự đoán cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bước tiếp theo, ta sẽ sử dụng *pooling* (ở đây sẽ là *max pooling*) ở đầu ra của *convolutional layers*. Hình dưới là hình minh họa của *max pooling*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](./images/4_maxpooling_figure4.PNG \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ý tưởng của *max pooling* là giá trị lớn nhất *có vai trò quan trọng hơn* trong việc xác định cảm xúc của bài đánh giá, tương ứng với **n-grams quan trọng hơn trong bài đánh giá**. Nhưng mà làm sao để biết n-grams nào quan trọng hơn? May mắn là với backpropagation, **weights** của filters sẽ thay đổi khi gặp các n-grams nhất định, biểu diễn cảm xúc của bài đánh giá rõ rệt hơn, do đó giá trị đầu ra của filter sẽ cao hơn. Giá trị cao này sẽ đi qua maxpooling nếu nó là giá trị cao nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do model của chúng ta có 100 filters với 3 kích thước khác nhau, có nghĩa chúng ta có 300 n-grams khác nhau mà model *nghĩ* nó là quan trọng. Chúng ta ghép **chúng** lại với nhau tạo thành *một single vector* và truyền vào *linear layers* để dự đoán. Ta có thể nghĩ *linear layer* này được \"tăng cường độ uy tín\" từ mỗi 300 n-grams và đưa ra kết quả cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta sẽ triển khai *convolutional layers* bằng `nn.Conv2d`. Đối số `in_channels` là số lượng kênh đầu vào, với ảnh màu (RGB), số kênh sẽ là 3, tuy nhiên ta sử dụng word embeddings nên chỉ có 1 kênh. Đối số `out_channels` là số lượng filters, còn `kernel_size` là kích thước của filters. Mỗi `kernel_size` có kích thước là **[n x emb_dim]** với n là số của n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong Pytorch, RNNs yêu cầu đầu vào có batch demension ở ví trí thứ 2, còn CNNs yêu cầu batch dimension ở vị trí đầu tiên - ta không cần hoán vị data do ta đã set `batch_first = True` trong `TEXT`. Sau đó, chúng ta sẽ truyền các sentence vào một embedding layer để lấy embeddings. Đối số thứ 2 trong `nn.Conv2d` phải là channel dimension. Tuy nhiên embeddings chỉ có 2 chiều, chưa có channel dimension nên ta dùng `unsqueeze` để tạo thêm channel dimension. Điều này khớp với tham số `in_channels = 1` khi khởi tạo *convolutional layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp đến, ta truyền tensors qua convolutional và pooling layers, sử dụng `ReLU` activation function sau convolutional layers. pooling layers có 1 đặc tính khá nice đó là nó có thể tự động xử lý các câu có chiều dài khác nhau. Kích thước đầu ra của convolutional layers phụ thuộc vào kích thước đầu vào, và batches khác nhau sẽ chứa các câu có độ dài khác nhau. Nếu không có maxpooling layer, đầu vào của linear layer sẽ phụ thuộc vào kích thước của câu đầu vào (và đương nhiên ta không muốn điều đó xảy ra). Có một phương pháp khác để xử lý vấn đề này là ta sẽ đệm các câu sao cho chúng có cùng độ dài. Tuy nhiên với maxpooling, ta luôn biết trước được kích thước đầu vào của linear layer và nó bằng số lượng filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " >**NOTE**: Có một ngoại lệ, xảy ra khi câu của ta ngắn hơn filters có kích thước lớn nhất. Trong trường hợp này, chúng ta cần đệm câu sao cho chúng có độ dài bằng với filters có size lớn nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cuối cùng ta thực hiện *dropout* ở vị trí đầu ra của filters đã được ghép nối với nhau và truyền qua linear layer để dự đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filters_size, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filters_size[0], embedding_dim))\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filters_size[1], embedding_dim))\n",
    "\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filters_size[2], embedding_dim))\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(len(filters_size) * n_filters, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        # embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "        conved0 = F.relu(self.conv_0(embedded))\n",
    "        conved1 = F.relu(self.conv_1(embedded))\n",
    "        conved2 = F.relu(self.conv_2(embedded))\n",
    "\n",
    "        # Do sử dụng CNNs và filter có kích thước [n x emb_dim] nên sau khi qua conv ta có chiều như sau:\n",
    "        # convedn = [batch size, n_filters, sent len - filter_sizes[n] + 1, 1]\n",
    "\n",
    "        conved0 = conved0.squeeze(3)\n",
    "        conved1 = conved1.squeeze(3)\n",
    "        conved2 = conved2.squeeze(3)\n",
    "\n",
    "        # convedn = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "\n",
    "        pooled0 = F.max_pool1d(conved0, conved0.shape[2]).squeeze(2)\n",
    "        pooled1 = F.max_pool1d(conved1, conved1.shape[2]).squeeze(2)\n",
    "        pooled2 = F.max_pool1d(conved2, conved2.shape[2]).squeeze(2)\n",
    "\n",
    "        # pooledn = [batch_size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat((pooled0, pooled1, pooled2), dim = 1))\n",
    "\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "\n",
    "        final_output = self.fc(cat)\n",
    "\n",
    "        # final_output = [batch_size, output_dim]\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Class `CNN` hiện tại chỉ có 3 lớp convolutional, tuy nhiên ta có thể cải thiện để model tổng quát hơn và nhận số lượng lớp convolutional bất kỳ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Để thực hiện được điều đó, ta đưa các convolutional vào một `nn.ModuleList`, đây là một hàm được dùng để lưu trữ danh sách các `nn.Module`s trong Pytorch. Nếu ta chỉ dùng `list` trong Python, các modules bên trong list sẽ không thể được nhìn thấy bởi bất kỳ module nào bên ngoài list đó => gây ra lỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thực tế, ta có thể sử dụng 1 - dimensional convolutional layers, khi đó embedding dimension sẽ là chiều sâu của filter và số lượng tokens trong câu sẽ là chiều rộng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,620,801 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1386,  0.1180,  0.3534,  ...,  0.1226,  0.5973, -0.1702],\n",
       "        [-0.0786,  0.0541, -0.0993,  ...,  0.2565, -0.1874, -0.4428],\n",
       "        [-0.3617,  0.6201,  0.1105,  ...,  0.2994, -0.5920,  1.0949]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khởi tạo weights bằng 0 cho các tokens chưa biết và các padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nhớ là dùng `model.eval()` khi test để tắt dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\longln3\\Anaconda3\\envs\\python3.7.6\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 6m 43s\n",
      "\tTrain Loss: 0.651 | Train Acc: 61.70%\n",
      "\t Val. Loss: 0.521 |  Val. Acc: 76.07%\n",
      "Epoch: 02 | Epoch Time: 7m 32s\n",
      "\tTrain Loss: 0.433 | Train Acc: 80.22%\n",
      "\t Val. Loss: 0.366 |  Val. Acc: 83.92%\n",
      "Epoch: 03 | Epoch Time: 6m 48s\n",
      "\tTrain Loss: 0.306 | Train Acc: 87.42%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 85.47%\n",
      "Epoch: 04 | Epoch Time: 6m 7s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.33%\n",
      "\t Val. Loss: 0.325 |  Val. Acc: 86.08%\n",
      "Epoch: 05 | Epoch Time: 6m 45s\n",
      "\tTrain Loss: 0.155 | Train Acc: 94.28%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 86.33%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.346 | Test Acc: 85.33%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Như đã đề cập từ trước, chiều dài của input sentence tối thiểu phải bằng với chiều cao của filters được sử dụng. Do đó ta điều chỉnh hàm `predict_sentiment` để có thể nhận đầu vào có kích thước nhỏ. Nếu *tokenized input sentence* có độ dài nhỏ hơn `min_len`, ta sẽ đệm thêm để nó có chiều dài bằng `min_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += [''] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09431809186935425"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9623662829399109"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
